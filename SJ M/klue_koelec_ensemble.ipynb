{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2293e003",
   "metadata": {},
   "source": [
    "### KLEU BERT-KoELECTRA Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88f4739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, ElectraTokenizer, BertForSequenceClassification, ElectraForSequenceClassification\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05dbbcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^가-힣a-z\\s]', ' ', text)\n",
    "    # Remove extra spaces\n",
    "    text = ' '.join(text.split())\n",
    "    # 한국어 불용어 리스트\n",
    "    stopwords = [\n",
    "        '이', '있', '하', '것', '들', '그', '되', '수', '이', '보', '않', '없', '나', '사람', '주', '아니', \n",
    "        '등', '같', '우리', '때', '년', '가', '한', '지', '대하', '오', '말', '일', '그렇', '위하', \n",
    "        '때문', '그것', '두', '말하', '알', '그러나', '받', '못하', '일', '그런', '또', '문제', '더', '사회', \n",
    "        '많', '그리고', '좋', '크', '따르', '중', '나오', '가지', '씨', '시키', '만들', '지금', '생각하', \n",
    "        '그러', '속', '하나', '집', '살', '모르', '적', '월', '데', '자신', '안', '어떤', '내', '내', '경우',\n",
    "        '명', '생각', '시간', '그녀', '다시', '이런', '앞', '보이', '번', '나', '다른', '어떻', '여자', '개',\n",
    "        '전', '들', '사실', '이렇', '점', '싶', '말', '정도', '좀', '원', '잘', '통하', '소리', '놓'\n",
    "    ]\n",
    "    # 불용어 제거\n",
    "    text = ' '.join(word for word in text.split() if word not in stopwords)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39ddc246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_json('./data/test.json').transpose()\n",
    "\n",
    "# train과 test 데이터의 텍스트 열 정규화\n",
    "train['conversation'] = train['conversation'].apply(clean_text)\n",
    "test['text'] = test['text'].apply(clean_text)\n",
    "\n",
    "# 지정된 클래스를 숫자로 인코딩\n",
    "label_dict = {\n",
    "    '협박 대화': 0,\n",
    "    '갈취 대화': 1,\n",
    "    '직장 내 괴롭힘 대화': 2,\n",
    "    '기타 괴롭힘 대화': 3\n",
    "}\n",
    "train['label_encoded'] = train['class'].map(label_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "455f797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KLUE용 Dataset 정의\n",
    "class KLUEDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe['conversation' if 'conversation' in dataframe else 'text'].tolist()\n",
    "        self.labels = dataframe['label_encoded'].tolist() if 'label_encoded' in dataframe else [0] * len(dataframe)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.labels[index], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# KoELECTRA용 Dataset 정의\n",
    "class KoELECTRADataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe['conversation' if 'conversation' in dataframe else 'text'].tolist()\n",
    "        self.labels = dataframe['label_encoded'].tolist() if 'label_encoded' in dataframe else [0] * len(dataframe)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.labels[index], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c32dd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader 설정\n",
    "MAX_LEN = 350\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "klue_tokenizer = BertTokenizer.from_pretrained('klue/bert-base')\n",
    "koelectra_tokenizer = ElectraTokenizer.from_pretrained('monologg/koelectra-base-v3-discriminator')\n",
    "\n",
    "klue_train_dataset = KLUEDataset(train, klue_tokenizer, MAX_LEN)\n",
    "koelectra_train_dataset = KoELECTRADataset(train, koelectra_tokenizer, MAX_LEN)\n",
    "\n",
    "klue_train_data_loader = DataLoader(klue_train_dataset, batch_size=BATCH_SIZE)\n",
    "koelectra_train_data_loader = DataLoader(koelectra_train_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "klue_test_dataset = KLUEDataset(test, klue_tokenizer, MAX_LEN)\n",
    "koelectra_test_dataset = KoELECTRADataset(test, koelectra_tokenizer, MAX_LEN)\n",
    "\n",
    "klue_test_data_loader = DataLoader(klue_test_dataset, batch_size=BATCH_SIZE)\n",
    "koelectra_test_data_loader = DataLoader(koelectra_test_dataset, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c5ab96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의\n",
    "class KLUEClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KLUEClass, self).__init__()\n",
    "        self.l1 = BertForSequenceClassification.from_pretrained('klue/bert-base', num_labels=4)\n",
    "        \n",
    "    def forward(self, ids, mask):\n",
    "        output = self.l1(ids, attention_mask=mask)\n",
    "        return output.logits\n",
    "\n",
    "class KoELECTRAClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KoELECTRAClass, self).__init__()\n",
    "        self.l1 = ElectraForSequenceClassification.from_pretrained('monologg/koelectra-base-v3-discriminator', num_labels=4)\n",
    "        \n",
    "    def forward(self, ids, mask):\n",
    "        output = self.l1(ids, attention_mask=mask)\n",
    "        return output.logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4a8c3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# KLUE 모델 로드하기\n",
    "# SAVE_PATH = './klue_model.pth'\n",
    "# klue_model = KLUEClass()\n",
    "# klue_model.load_state_dict(torch.load(SAVE_PATH), strict=False)\n",
    "# klue_model.to(torch.device(\"cuda\"))\n",
    "# klue_model.eval()  # 항상 eval 모드로 설정한 후 예측을 수행해야 합니다.\n",
    "\n",
    "# KoELECTRA 모델 로드하기\n",
    "# SAVE_PATH = './koelectra_model.pth'\n",
    "# koelectra_model = KoELECTRAClass()\n",
    "# loaded_koelectra_model.load_state_dict(torch.load(SAVE_PATH), strict=False)\n",
    "# loaded_koelectra_model.to(torch.device(\"cuda\"))\n",
    "# loaded_koelectra_model.eval()\n",
    "\n",
    "# KLUE 모델 인스턴스 생성\n",
    "klue_model = KLUEClass()\n",
    "klue_model.to(torch.device(\"cuda\"))\n",
    "\n",
    "# KoELECTRA 모델 인스턴스 생성\n",
    "koelectra_model = KoELECTRAClass()\n",
    "koelectra_model.to(torch.device(\"cuda\"))\n",
    "\n",
    "\n",
    "# 손실 함수 및 최적화 함수 설정\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "klue_optimizer = torch.optim.Adam(params=klue_model.parameters(), lr=1e-5)\n",
    "koelectra_optimizer = torch.optim.Adam(params=koelectra_model.parameters(), lr=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d2975d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 함수 정의\n",
    "def train(model, tokenizer, optimizer, data_loader, epoch):\n",
    "    model.train()\n",
    "    for _, data in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "        ids = data['ids'].to(torch.device(\"cuda\"), dtype=torch.long)\n",
    "        mask = data['mask'].to(torch.device(\"cuda\"), dtype=torch.long)\n",
    "        targets = data['targets'].to(torch.device(\"cuda\"), dtype=torch.long)\n",
    "\n",
    "        outputs = model(ids, mask)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1633cb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training KoELECTRA model, Epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e63dc13b3a694deebadad8d6c08241a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/988 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training KoELECTRA model, Epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c0e944895e4bc8bebf687eb3da7596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/988 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training KoELECTRA model, Epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70ff56bd6b734a2daa3e59184633619c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/988 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training KoELECTRA model, Epoch: 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3129280d7219474aaf51d4d954fa9ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/988 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 학습 실행\n",
    "KLUE_EPOCHS = 2 \n",
    "KOELECTRA_EPOCHS = 4\n",
    "\n",
    "for epoch in range(KLUE_EPOCHS):\n",
    "    print(\"Training KLUE model, Epoch:\", epoch)\n",
    "    train(klue_model, klue_tokenizer, klue_optimizer, klue_train_data_loader, epoch)\n",
    "\n",
    "for epoch in range(KOELECTRA_EPOCHS):\n",
    "    print(\"Training KoELECTRA model, Epoch:\", epoch)\n",
    "    train(koelectra_model, koelectra_tokenizer, koelectra_optimizer, koelectra_train_data_loader, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "868fa9b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c927e9fd2046acae6e18952f9a3e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a01a5c725e3441593d38b776755e8ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 확률 예측 함수 정의\n",
    "def predict_proba(model, data_loader):\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
    "            ids = data['ids'].to(torch.device(\"cuda\"), dtype=torch.long)\n",
    "            mask = data['mask'].to(torch.device(\"cuda\"), dtype=torch.long)\n",
    "            \n",
    "            outputs = model(ids, mask)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            all_probs.extend(probs.cpu().numpy().tolist())\n",
    "    return all_probs\n",
    "\n",
    "# 각 모델로 확률 예측 실행\n",
    "klue_probs = predict_proba(klue_model, klue_test_data_loader)\n",
    "koelectra_probs = predict_proba(koelectra_model, koelectra_test_data_loader)\n",
    "\n",
    "# KLUE 예측값을 submission.csv로 저장\n",
    "klue_predictions = [np.argmax(prob) for prob in klue_probs]\n",
    "klue_submission = pd.DataFrame({'file_name': test.index, 'class': klue_predictions})\n",
    "klue_submission.to_csv('./klue_submission.csv', index=False)\n",
    "\n",
    "# Koelectra 예측값을 submission.csv로 저장\n",
    "koelectra_predictions = [np.argmax(prob) for prob in koelectra_probs]\n",
    "koelectra_submission = pd.DataFrame({'file_name': test.index, 'class': koelectra_predictions})\n",
    "koelectra_submission.to_csv('./koelectra_submission.csv', index=False)\n",
    "\n",
    "# 확률을 평균내어 최종 클래스 결정\n",
    "final_predictions = []\n",
    "for klue_prob, k_electra_prob in zip(klue_probs, koelectra_probs):\n",
    "    avg_prob = [(a+b)/2 for a, b in zip(klue_prob, k_electra_prob)]\n",
    "    final_predictions.append(np.argmax(avg_prob))\n",
    "\n",
    "# 앙상블 결과를 submission.csv로 저장\n",
    "ensemble_submission = pd.DataFrame({'file_name': test.index, 'class': final_predictions})\n",
    "ensemble_submission.to_csv('./ensemble_submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f29c78f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KLUE 모델 저장하기\n",
    "#SAVE_PATH = './klue_model.pth'\n",
    "#torch.save(klue_model.state_dict(), SAVE_PATH)\n",
    "\n",
    "# Ko-Electra 모델 저장하기\n",
    "#SAVE_PATH_el = './koelectra_model.pth'\n",
    "#torch.save(koelectra_model.state_dict(), SAVE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "441e9bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
