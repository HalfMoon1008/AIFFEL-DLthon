{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae0c47e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9791dfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, ElectraTokenizer, BertForSequenceClassification, ElectraForSequenceClassification\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7f7d380",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^가-힣0-9]', ' ', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # 한국어 불용어 리스트\n",
    "    stopwords = [\n",
    "        '이', '있', '하', '것', '들', '그', '되', '수', '이', '보', '않', '없', '나', '사람', '주', '아니', \n",
    "        '등', '같', '우리', '때', '년', '가', '한', '지', '대하', '오', '말', '일', '그렇', '위하', \n",
    "        '때문', '그것', '두', '말하', '알', '그러나', '받', '못하', '일', '그런', '또', '문제', '더', '사회', \n",
    "        '많', '그리고', '좋', '크', '따르', '중', '나오', '가지', '씨', '시키', '만들', '지금', '생각하', \n",
    "        '그러', '속', '하나', '집', '살', '모르', '적', '월', '데', '자신', '안', '어떤', '내', '내', '경우',\n",
    "        '명', '생각', '시간', '그녀', '다시', '이런', '앞', '보이', '번', '나', '다른', '어떻', '여자', '개',\n",
    "        '전', '들', '사실', '이렇', '점', '싶', '말', '정도', '좀', '원', '잘', '통하', '소리', '놓'\n",
    "    ]\n",
    "    \n",
    "    # 불용어 제거\n",
    "    text = ' '.join(word for word in text.split() if word not in stopwords)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23e04ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the conversation column names for each file based on the earlier inspection\n",
    "conversation_columns = {\n",
    "    \"./data/train_sr_cleaned.csv\": \"conversation_sr_cleaned\",\n",
    "    \"./data/train_augmented_wv_.csv\": \"conversation\",\n",
    "    \"./data/LLaMa2_Augmentation_trian.csv\": \"conversation\",\n",
    "    \"./data/train.csv\": \"conversation\"\n",
    "}\n",
    "\n",
    "# Load and preprocess the data from each file, then concatenate them\n",
    "all_dataframes = []\n",
    "\n",
    "for file_path, conv_column in conversation_columns.items():\n",
    "    df_temp = pd.read_csv(file_path)\n",
    "    df_temp = df_temp[['class', conv_column]]\n",
    "    df_temp.columns = ['class', 'conversation']  # Renaming columns for uniformity\n",
    "    df_temp['conversation'] = df_temp['conversation'].apply(clean_text)\n",
    "    all_dataframes.append(df_temp)\n",
    "\n",
    "# Concatenate all the dataframes\n",
    "merged_data = pd.concat(all_dataframes, ignore_index=True)\n",
    "\n",
    "test = pd.read_json('./data/test.json').transpose()\n",
    "\n",
    "# train 데이터의 텍스트 열 정규화\n",
    "merged_data['conversation'] = merged_data['conversation'].apply(clean_text)\n",
    "test['conversation'] = test['text'].apply(clean_text)\n",
    "\n",
    "# 지정된 클래스를 숫자로 인코딩\n",
    "label_dict = {\n",
    "    '협박 대화': 0,\n",
    "    '갈취 대화': 1,\n",
    "    '직장 내 괴롭힘 대화': 2,\n",
    "    '기타 괴롭힘 대화': 3\n",
    "}\n",
    "merged_data['label_encoded'] = merged_data['class'].map(label_dict)\n",
    "\n",
    "merged_data.drop_duplicates(subset=['conversation'], inplace=True)\n",
    "\n",
    "\n",
    "# NaN 값을 가진 행 제거\n",
    "merged_data.dropna(subset=['conversation', 'label_encoded'], inplace=True)\n",
    "\n",
    "# 빈 문자열 값을 가진 행 제거\n",
    "merged_data = merged_data[merged_data['conversation'] != \"\"]\n",
    "\n",
    "merged_data.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e92e24f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       class                                       conversation  label_encoded\n",
      "0      협박 대화  너 스스로를 죽여달라고 애원하는 인가 아닙니다 죄송합니다 죽을 거면 혼자 죽지 사건...              0\n",
      "1      협박 대화  길동판단입니다꽤 시 분 마트에 폭발물을 설치할거다 죽는거야 똑바로 들어 한번만 얘기...              0\n",
      "2  기타 괴롭힘 대화  너 되게 귀여운거 알지 나보다 작은 남자는 첨봤어 그만해 니들 놀리는거 재미없어 지...              3\n",
      "3      갈취 대화  어이 거기 예 너 말이야 너 동안 이리 오라고 사이 무슨 너 늑대 옷 좋아보인다 얘...              1\n",
      "4      갈취 대화  저기요 혹시 한데이 너무 뜨겁잖아요 저희 회사에서 알아보던 선크림 파는데 손등에 발...              1\n"
     ]
    }
   ],
   "source": [
    "print(merged_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b87f0339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22da5404213041a4a57c0fc9af3408f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/336k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07178af2065f42e3b5088d62be3f7560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/153 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd8bf7f6fc841f7a95945c96a7b1479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb9f5deebf44732bd1b9869b229a51a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/725 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 토크나이저 로드\n",
    "from transformers import FunnelTokenizerFast, FunnelForSequenceClassification\n",
    "\n",
    "funnel_tokenizer = FunnelTokenizerFast.from_pretrained(\"kykim/funnel-kor-base\")\n",
    "\n",
    "# funnel용 Dataset 정의\n",
    "class FunnelDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe['conversation' if 'conversation' in dataframe else 'text'].tolist()\n",
    "        self.labels = dataframe['label_encoded'].tolist() if 'label_encoded' in dataframe else [0] * len(dataframe)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            text = str(self.text[index])\n",
    "            inputs = self.tokenizer.encode_plus(\n",
    "                text,\n",
    "                None,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_len,\n",
    "                pad_to_max_length=True,\n",
    "                return_token_type_ids=True\n",
    "            )\n",
    "            ids = inputs['input_ids']\n",
    "            mask = inputs['attention_mask']\n",
    "\n",
    "            return {\n",
    "                'ids': torch.tensor(ids, dtype=torch.long),\n",
    "                'mask': torch.tensor(mask, dtype=torch.long),\n",
    "                'targets': torch.tensor(self.labels[index], dtype=torch.long)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error at index {index}: {e}\")\n",
    "            print(f\"Text: {self.text[index]}, Label: {self.labels[index]}\")\n",
    "            raise\n",
    "\n",
    "class FunnelKorBaseClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FunnelKorBaseClass, self).__init__()\n",
    "        self.l1 = FunnelForSequenceClassification.from_pretrained('kykim/funnel-kor-base', num_labels=4)\n",
    "        \n",
    "    def forward(self, ids, mask):\n",
    "        outputs = self.l1(ids, attention_mask=mask, return_dict=True)\n",
    "        return outputs.logits\n",
    "\n",
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(data_loader):\n",
    "            ids = data['ids'].to(torch.device(\"cuda\"), dtype=torch.long)\n",
    "            mask = data['mask'].to(torch.device(\"cuda\"), dtype=torch.long)\n",
    "            \n",
    "            outputs = model(ids, mask)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ae388b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Overall Progress:   0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2452ff078ff415d9a2b898c3f2e0fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/709M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kykim/funnel-kor-base were not used when initializing FunnelForSequenceClassification: ['decoder.layers.1.attention.post_proj.bias', 'decoder.layers.0.ffn.linear_1.weight', 'decoder.layers.1.attention.seg_embed', 'decoder.layers.1.attention.k_head.bias', 'decoder.layers.0.attention.r_r_bias', 'decoder.layers.1.attention.r_r_bias', 'decoder.layers.0.attention.layer_norm.weight', 'decoder.layers.0.attention.layer_norm.bias', 'decoder.layers.1.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.weight', 'decoder.layers.0.attention.r_w_bias', 'decoder.layers.0.ffn.linear_2.bias', 'decoder.layers.0.attention.seg_embed', 'decoder.layers.1.ffn.linear_1.weight', 'decoder.layers.0.attention.q_head.weight', 'decoder.layers.1.attention.r_s_bias', 'decoder.layers.1.attention.k_head.weight', 'decoder.layers.0.attention.post_proj.bias', 'decoder.layers.0.attention.k_head.bias', 'decoder.layers.1.attention.layer_norm.weight', 'decoder.layers.0.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.bias', 'decoder.layers.1.attention.q_head.weight', 'decoder.layers.1.attention.r_w_bias', 'decoder.layers.1.attention.r_kernel', 'decoder.layers.1.ffn.layer_norm.weight', 'decoder.layers.0.ffn.linear_2.weight', 'decoder.layers.0.ffn.layer_norm.weight', 'decoder.layers.0.attention.k_head.weight', 'decoder.layers.1.attention.v_head.weight', 'decoder.layers.0.attention.post_proj.weight', 'decoder.layers.1.ffn.linear_2.weight', 'decoder.layers.1.attention.v_head.bias', 'decoder.layers.1.ffn.linear_2.bias', 'decoder.layers.0.ffn.linear_1.bias', 'decoder.layers.1.ffn.linear_1.bias', 'decoder.layers.0.attention.r_s_bias', 'decoder.layers.0.attention.r_kernel', 'decoder.layers.1.attention.post_proj.weight', 'decoder.layers.1.attention.layer_norm.bias']\n",
      "- This IS expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FunnelForSequenceClassification were not initialized from the model checkpoint at kykim/funnel-kor-base and are newly initialized: ['classifier.linear_hidden.weight', 'classifier.linear_out.weight', 'classifier.linear_out.bias', 'classifier.linear_hidden.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Training for MAX_LEN=128:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "\n",
      "Training for MAX_LEN=128:  50%|█████     | 1/2 [06:52<06:52, 412.59s/it]\u001b[A\n",
      "Training for MAX_LEN=128: 100%|██████████| 2/2 [13:46<00:00, 413.47s/it]\u001b[A\n",
      "Overall Progress:   4%|▍         | 1/23 [15:07<5:32:42, 907.38s/it]     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.8925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kykim/funnel-kor-base were not used when initializing FunnelForSequenceClassification: ['decoder.layers.1.attention.post_proj.bias', 'decoder.layers.0.ffn.linear_1.weight', 'decoder.layers.1.attention.seg_embed', 'decoder.layers.1.attention.k_head.bias', 'decoder.layers.0.attention.r_r_bias', 'decoder.layers.1.attention.r_r_bias', 'decoder.layers.0.attention.layer_norm.weight', 'decoder.layers.0.attention.layer_norm.bias', 'decoder.layers.1.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.weight', 'decoder.layers.0.attention.r_w_bias', 'decoder.layers.0.ffn.linear_2.bias', 'decoder.layers.0.attention.seg_embed', 'decoder.layers.1.ffn.linear_1.weight', 'decoder.layers.0.attention.q_head.weight', 'decoder.layers.1.attention.r_s_bias', 'decoder.layers.1.attention.k_head.weight', 'decoder.layers.0.attention.post_proj.bias', 'decoder.layers.0.attention.k_head.bias', 'decoder.layers.1.attention.layer_norm.weight', 'decoder.layers.0.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.bias', 'decoder.layers.1.attention.q_head.weight', 'decoder.layers.1.attention.r_w_bias', 'decoder.layers.1.attention.r_kernel', 'decoder.layers.1.ffn.layer_norm.weight', 'decoder.layers.0.ffn.linear_2.weight', 'decoder.layers.0.ffn.layer_norm.weight', 'decoder.layers.0.attention.k_head.weight', 'decoder.layers.1.attention.v_head.weight', 'decoder.layers.0.attention.post_proj.weight', 'decoder.layers.1.ffn.linear_2.weight', 'decoder.layers.1.attention.v_head.bias', 'decoder.layers.1.ffn.linear_2.bias', 'decoder.layers.0.ffn.linear_1.bias', 'decoder.layers.1.ffn.linear_1.bias', 'decoder.layers.0.attention.r_s_bias', 'decoder.layers.0.attention.r_kernel', 'decoder.layers.1.attention.post_proj.weight', 'decoder.layers.1.attention.layer_norm.bias']\n",
      "- This IS expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FunnelForSequenceClassification were not initialized from the model checkpoint at kykim/funnel-kor-base and are newly initialized: ['classifier.linear_hidden.weight', 'classifier.linear_out.weight', 'classifier.linear_out.bias', 'classifier.linear_hidden.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Training for MAX_LEN=144:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Training for MAX_LEN=144:  50%|█████     | 1/2 [07:45<07:45, 465.52s/it]\u001b[A\n",
      "Training for MAX_LEN=144: 100%|██████████| 2/2 [15:31<00:00, 465.78s/it]\u001b[A\n",
      "Overall Progress:   9%|▊         | 2/23 [30:45<5:23:52, 925.37s/it]     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.8925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kykim/funnel-kor-base were not used when initializing FunnelForSequenceClassification: ['decoder.layers.1.attention.post_proj.bias', 'decoder.layers.0.ffn.linear_1.weight', 'decoder.layers.1.attention.seg_embed', 'decoder.layers.1.attention.k_head.bias', 'decoder.layers.0.attention.r_r_bias', 'decoder.layers.1.attention.r_r_bias', 'decoder.layers.0.attention.layer_norm.weight', 'decoder.layers.0.attention.layer_norm.bias', 'decoder.layers.1.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.weight', 'decoder.layers.0.attention.r_w_bias', 'decoder.layers.0.ffn.linear_2.bias', 'decoder.layers.0.attention.seg_embed', 'decoder.layers.1.ffn.linear_1.weight', 'decoder.layers.0.attention.q_head.weight', 'decoder.layers.1.attention.r_s_bias', 'decoder.layers.1.attention.k_head.weight', 'decoder.layers.0.attention.post_proj.bias', 'decoder.layers.0.attention.k_head.bias', 'decoder.layers.1.attention.layer_norm.weight', 'decoder.layers.0.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.bias', 'decoder.layers.1.attention.q_head.weight', 'decoder.layers.1.attention.r_w_bias', 'decoder.layers.1.attention.r_kernel', 'decoder.layers.1.ffn.layer_norm.weight', 'decoder.layers.0.ffn.linear_2.weight', 'decoder.layers.0.ffn.layer_norm.weight', 'decoder.layers.0.attention.k_head.weight', 'decoder.layers.1.attention.v_head.weight', 'decoder.layers.0.attention.post_proj.weight', 'decoder.layers.1.ffn.linear_2.weight', 'decoder.layers.1.attention.v_head.bias', 'decoder.layers.1.ffn.linear_2.bias', 'decoder.layers.0.ffn.linear_1.bias', 'decoder.layers.1.ffn.linear_1.bias', 'decoder.layers.0.attention.r_s_bias', 'decoder.layers.0.attention.r_kernel', 'decoder.layers.1.attention.post_proj.weight', 'decoder.layers.1.attention.layer_norm.bias']\n",
      "- This IS expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FunnelForSequenceClassification were not initialized from the model checkpoint at kykim/funnel-kor-base and are newly initialized: ['classifier.linear_hidden.weight', 'classifier.linear_out.weight', 'classifier.linear_out.bias', 'classifier.linear_hidden.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Training for MAX_LEN=160:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Training for MAX_LEN=160:  50%|█████     | 1/2 [08:23<08:23, 503.85s/it]\u001b[A\n",
      "Training for MAX_LEN=160: 100%|██████████| 2/2 [16:47<00:00, 503.81s/it]\u001b[A\n",
      "Overall Progress:  13%|█▎        | 3/23 [47:39<5:21:57, 965.87s/it]     \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.8925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kykim/funnel-kor-base were not used when initializing FunnelForSequenceClassification: ['decoder.layers.1.attention.post_proj.bias', 'decoder.layers.0.ffn.linear_1.weight', 'decoder.layers.1.attention.seg_embed', 'decoder.layers.1.attention.k_head.bias', 'decoder.layers.0.attention.r_r_bias', 'decoder.layers.1.attention.r_r_bias', 'decoder.layers.0.attention.layer_norm.weight', 'decoder.layers.0.attention.layer_norm.bias', 'decoder.layers.1.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.weight', 'decoder.layers.0.attention.r_w_bias', 'decoder.layers.0.ffn.linear_2.bias', 'decoder.layers.0.attention.seg_embed', 'decoder.layers.1.ffn.linear_1.weight', 'decoder.layers.0.attention.q_head.weight', 'decoder.layers.1.attention.r_s_bias', 'decoder.layers.1.attention.k_head.weight', 'decoder.layers.0.attention.post_proj.bias', 'decoder.layers.0.attention.k_head.bias', 'decoder.layers.1.attention.layer_norm.weight', 'decoder.layers.0.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.bias', 'decoder.layers.1.attention.q_head.weight', 'decoder.layers.1.attention.r_w_bias', 'decoder.layers.1.attention.r_kernel', 'decoder.layers.1.ffn.layer_norm.weight', 'decoder.layers.0.ffn.linear_2.weight', 'decoder.layers.0.ffn.layer_norm.weight', 'decoder.layers.0.attention.k_head.weight', 'decoder.layers.1.attention.v_head.weight', 'decoder.layers.0.attention.post_proj.weight', 'decoder.layers.1.ffn.linear_2.weight', 'decoder.layers.1.attention.v_head.bias', 'decoder.layers.1.ffn.linear_2.bias', 'decoder.layers.0.ffn.linear_1.bias', 'decoder.layers.1.ffn.linear_1.bias', 'decoder.layers.0.attention.r_s_bias', 'decoder.layers.0.attention.r_kernel', 'decoder.layers.1.attention.post_proj.weight', 'decoder.layers.1.attention.layer_norm.bias']\n",
      "- This IS expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FunnelForSequenceClassification were not initialized from the model checkpoint at kykim/funnel-kor-base and are newly initialized: ['classifier.linear_hidden.weight', 'classifier.linear_out.weight', 'classifier.linear_out.bias', 'classifier.linear_hidden.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Training for MAX_LEN=176:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Training for MAX_LEN=176:  50%|█████     | 1/2 [09:25<09:25, 565.72s/it]\u001b[A\n",
      "Training for MAX_LEN=176: 100%|██████████| 2/2 [18:51<00:00, 565.86s/it]\u001b[A\n",
      "Overall Progress:  17%|█▋        | 4/23 [1:06:38<5:27:29, 1034.18s/it]  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kykim/funnel-kor-base were not used when initializing FunnelForSequenceClassification: ['decoder.layers.1.attention.post_proj.bias', 'decoder.layers.0.ffn.linear_1.weight', 'decoder.layers.1.attention.seg_embed', 'decoder.layers.1.attention.k_head.bias', 'decoder.layers.0.attention.r_r_bias', 'decoder.layers.1.attention.r_r_bias', 'decoder.layers.0.attention.layer_norm.weight', 'decoder.layers.0.attention.layer_norm.bias', 'decoder.layers.1.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.weight', 'decoder.layers.0.attention.r_w_bias', 'decoder.layers.0.ffn.linear_2.bias', 'decoder.layers.0.attention.seg_embed', 'decoder.layers.1.ffn.linear_1.weight', 'decoder.layers.0.attention.q_head.weight', 'decoder.layers.1.attention.r_s_bias', 'decoder.layers.1.attention.k_head.weight', 'decoder.layers.0.attention.post_proj.bias', 'decoder.layers.0.attention.k_head.bias', 'decoder.layers.1.attention.layer_norm.weight', 'decoder.layers.0.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.bias', 'decoder.layers.1.attention.q_head.weight', 'decoder.layers.1.attention.r_w_bias', 'decoder.layers.1.attention.r_kernel', 'decoder.layers.1.ffn.layer_norm.weight', 'decoder.layers.0.ffn.linear_2.weight', 'decoder.layers.0.ffn.layer_norm.weight', 'decoder.layers.0.attention.k_head.weight', 'decoder.layers.1.attention.v_head.weight', 'decoder.layers.0.attention.post_proj.weight', 'decoder.layers.1.ffn.linear_2.weight', 'decoder.layers.1.attention.v_head.bias', 'decoder.layers.1.ffn.linear_2.bias', 'decoder.layers.0.ffn.linear_1.bias', 'decoder.layers.1.ffn.linear_1.bias', 'decoder.layers.0.attention.r_s_bias', 'decoder.layers.0.attention.r_kernel', 'decoder.layers.1.attention.post_proj.weight', 'decoder.layers.1.attention.layer_norm.bias']\n",
      "- This IS expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FunnelForSequenceClassification were not initialized from the model checkpoint at kykim/funnel-kor-base and are newly initialized: ['classifier.linear_hidden.weight', 'classifier.linear_out.weight', 'classifier.linear_out.bias', 'classifier.linear_hidden.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Training for MAX_LEN=192:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Training for MAX_LEN=192:  50%|█████     | 1/2 [10:08<10:08, 608.96s/it]\u001b[A\n",
      "Training for MAX_LEN=192: 100%|██████████| 2/2 [20:18<00:00, 609.01s/it]\u001b[A\n",
      "Overall Progress:  22%|██▏       | 5/23 [1:27:03<5:30:56, 1103.12s/it]  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kykim/funnel-kor-base were not used when initializing FunnelForSequenceClassification: ['decoder.layers.1.attention.post_proj.bias', 'decoder.layers.0.ffn.linear_1.weight', 'decoder.layers.1.attention.seg_embed', 'decoder.layers.1.attention.k_head.bias', 'decoder.layers.0.attention.r_r_bias', 'decoder.layers.1.attention.r_r_bias', 'decoder.layers.0.attention.layer_norm.weight', 'decoder.layers.0.attention.layer_norm.bias', 'decoder.layers.1.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.weight', 'decoder.layers.0.attention.r_w_bias', 'decoder.layers.0.ffn.linear_2.bias', 'decoder.layers.0.attention.seg_embed', 'decoder.layers.1.ffn.linear_1.weight', 'decoder.layers.0.attention.q_head.weight', 'decoder.layers.1.attention.r_s_bias', 'decoder.layers.1.attention.k_head.weight', 'decoder.layers.0.attention.post_proj.bias', 'decoder.layers.0.attention.k_head.bias', 'decoder.layers.1.attention.layer_norm.weight', 'decoder.layers.0.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.bias', 'decoder.layers.1.attention.q_head.weight', 'decoder.layers.1.attention.r_w_bias', 'decoder.layers.1.attention.r_kernel', 'decoder.layers.1.ffn.layer_norm.weight', 'decoder.layers.0.ffn.linear_2.weight', 'decoder.layers.0.ffn.layer_norm.weight', 'decoder.layers.0.attention.k_head.weight', 'decoder.layers.1.attention.v_head.weight', 'decoder.layers.0.attention.post_proj.weight', 'decoder.layers.1.ffn.linear_2.weight', 'decoder.layers.1.attention.v_head.bias', 'decoder.layers.1.ffn.linear_2.bias', 'decoder.layers.0.ffn.linear_1.bias', 'decoder.layers.1.ffn.linear_1.bias', 'decoder.layers.0.attention.r_s_bias', 'decoder.layers.0.attention.r_kernel', 'decoder.layers.1.attention.post_proj.weight', 'decoder.layers.1.attention.layer_norm.bias']\n",
      "- This IS expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FunnelForSequenceClassification were not initialized from the model checkpoint at kykim/funnel-kor-base and are newly initialized: ['classifier.linear_hidden.weight', 'classifier.linear_out.weight', 'classifier.linear_out.bias', 'classifier.linear_hidden.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Training for MAX_LEN=208:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Training for MAX_LEN=208:  50%|█████     | 1/2 [10:57<10:57, 657.93s/it]\u001b[A\n",
      "Training for MAX_LEN=208: 100%|██████████| 2/2 [21:55<00:00, 657.65s/it]\u001b[A\n",
      "Overall Progress:  26%|██▌       | 6/23 [1:49:06<5:33:44, 1177.92s/it]  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kykim/funnel-kor-base were not used when initializing FunnelForSequenceClassification: ['decoder.layers.1.attention.post_proj.bias', 'decoder.layers.0.ffn.linear_1.weight', 'decoder.layers.1.attention.seg_embed', 'decoder.layers.1.attention.k_head.bias', 'decoder.layers.0.attention.r_r_bias', 'decoder.layers.1.attention.r_r_bias', 'decoder.layers.0.attention.layer_norm.weight', 'decoder.layers.0.attention.layer_norm.bias', 'decoder.layers.1.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.weight', 'decoder.layers.0.attention.r_w_bias', 'decoder.layers.0.ffn.linear_2.bias', 'decoder.layers.0.attention.seg_embed', 'decoder.layers.1.ffn.linear_1.weight', 'decoder.layers.0.attention.q_head.weight', 'decoder.layers.1.attention.r_s_bias', 'decoder.layers.1.attention.k_head.weight', 'decoder.layers.0.attention.post_proj.bias', 'decoder.layers.0.attention.k_head.bias', 'decoder.layers.1.attention.layer_norm.weight', 'decoder.layers.0.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.bias', 'decoder.layers.1.attention.q_head.weight', 'decoder.layers.1.attention.r_w_bias', 'decoder.layers.1.attention.r_kernel', 'decoder.layers.1.ffn.layer_norm.weight', 'decoder.layers.0.ffn.linear_2.weight', 'decoder.layers.0.ffn.layer_norm.weight', 'decoder.layers.0.attention.k_head.weight', 'decoder.layers.1.attention.v_head.weight', 'decoder.layers.0.attention.post_proj.weight', 'decoder.layers.1.ffn.linear_2.weight', 'decoder.layers.1.attention.v_head.bias', 'decoder.layers.1.ffn.linear_2.bias', 'decoder.layers.0.ffn.linear_1.bias', 'decoder.layers.1.ffn.linear_1.bias', 'decoder.layers.0.attention.r_s_bias', 'decoder.layers.0.attention.r_kernel', 'decoder.layers.1.attention.post_proj.weight', 'decoder.layers.1.attention.layer_norm.bias']\n",
      "- This IS expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FunnelForSequenceClassification were not initialized from the model checkpoint at kykim/funnel-kor-base and are newly initialized: ['classifier.linear_hidden.weight', 'classifier.linear_out.weight', 'classifier.linear_out.bias', 'classifier.linear_hidden.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Training for MAX_LEN=224:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Training for MAX_LEN=224:  50%|█████     | 1/2 [11:54<11:54, 714.76s/it]\u001b[A\n",
      "Training for MAX_LEN=224: 100%|██████████| 2/2 [23:49<00:00, 714.93s/it]\u001b[A\n",
      "Overall Progress:  30%|███       | 7/23 [2:13:04<5:36:48, 1263.01s/it]  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.9025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kykim/funnel-kor-base were not used when initializing FunnelForSequenceClassification: ['decoder.layers.1.attention.post_proj.bias', 'decoder.layers.0.ffn.linear_1.weight', 'decoder.layers.1.attention.seg_embed', 'decoder.layers.1.attention.k_head.bias', 'decoder.layers.0.attention.r_r_bias', 'decoder.layers.1.attention.r_r_bias', 'decoder.layers.0.attention.layer_norm.weight', 'decoder.layers.0.attention.layer_norm.bias', 'decoder.layers.1.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.weight', 'decoder.layers.0.attention.r_w_bias', 'decoder.layers.0.ffn.linear_2.bias', 'decoder.layers.0.attention.seg_embed', 'decoder.layers.1.ffn.linear_1.weight', 'decoder.layers.0.attention.q_head.weight', 'decoder.layers.1.attention.r_s_bias', 'decoder.layers.1.attention.k_head.weight', 'decoder.layers.0.attention.post_proj.bias', 'decoder.layers.0.attention.k_head.bias', 'decoder.layers.1.attention.layer_norm.weight', 'decoder.layers.0.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.bias', 'decoder.layers.1.attention.q_head.weight', 'decoder.layers.1.attention.r_w_bias', 'decoder.layers.1.attention.r_kernel', 'decoder.layers.1.ffn.layer_norm.weight', 'decoder.layers.0.ffn.linear_2.weight', 'decoder.layers.0.ffn.layer_norm.weight', 'decoder.layers.0.attention.k_head.weight', 'decoder.layers.1.attention.v_head.weight', 'decoder.layers.0.attention.post_proj.weight', 'decoder.layers.1.ffn.linear_2.weight', 'decoder.layers.1.attention.v_head.bias', 'decoder.layers.1.ffn.linear_2.bias', 'decoder.layers.0.ffn.linear_1.bias', 'decoder.layers.1.ffn.linear_1.bias', 'decoder.layers.0.attention.r_s_bias', 'decoder.layers.0.attention.r_kernel', 'decoder.layers.1.attention.post_proj.weight', 'decoder.layers.1.attention.layer_norm.bias']\n",
      "- This IS expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FunnelForSequenceClassification were not initialized from the model checkpoint at kykim/funnel-kor-base and are newly initialized: ['classifier.linear_hidden.weight', 'classifier.linear_out.weight', 'classifier.linear_out.bias', 'classifier.linear_hidden.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Training for MAX_LEN=240:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Training for MAX_LEN=240:  50%|█████     | 1/2 [12:31<12:31, 751.31s/it]\u001b[A\n",
      "Training for MAX_LEN=240: 100%|██████████| 2/2 [25:02<00:00, 751.00s/it]\u001b[A\n",
      "Overall Progress:  35%|███▍      | 8/23 [2:38:16<5:35:29, 1341.97s/it]  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kykim/funnel-kor-base were not used when initializing FunnelForSequenceClassification: ['decoder.layers.1.attention.post_proj.bias', 'decoder.layers.0.ffn.linear_1.weight', 'decoder.layers.1.attention.seg_embed', 'decoder.layers.1.attention.k_head.bias', 'decoder.layers.0.attention.r_r_bias', 'decoder.layers.1.attention.r_r_bias', 'decoder.layers.0.attention.layer_norm.weight', 'decoder.layers.0.attention.layer_norm.bias', 'decoder.layers.1.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.weight', 'decoder.layers.0.attention.r_w_bias', 'decoder.layers.0.ffn.linear_2.bias', 'decoder.layers.0.attention.seg_embed', 'decoder.layers.1.ffn.linear_1.weight', 'decoder.layers.0.attention.q_head.weight', 'decoder.layers.1.attention.r_s_bias', 'decoder.layers.1.attention.k_head.weight', 'decoder.layers.0.attention.post_proj.bias', 'decoder.layers.0.attention.k_head.bias', 'decoder.layers.1.attention.layer_norm.weight', 'decoder.layers.0.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.bias', 'decoder.layers.1.attention.q_head.weight', 'decoder.layers.1.attention.r_w_bias', 'decoder.layers.1.attention.r_kernel', 'decoder.layers.1.ffn.layer_norm.weight', 'decoder.layers.0.ffn.linear_2.weight', 'decoder.layers.0.ffn.layer_norm.weight', 'decoder.layers.0.attention.k_head.weight', 'decoder.layers.1.attention.v_head.weight', 'decoder.layers.0.attention.post_proj.weight', 'decoder.layers.1.ffn.linear_2.weight', 'decoder.layers.1.attention.v_head.bias', 'decoder.layers.1.ffn.linear_2.bias', 'decoder.layers.0.ffn.linear_1.bias', 'decoder.layers.1.ffn.linear_1.bias', 'decoder.layers.0.attention.r_s_bias', 'decoder.layers.0.attention.r_kernel', 'decoder.layers.1.attention.post_proj.weight', 'decoder.layers.1.attention.layer_norm.bias']\n",
      "- This IS expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FunnelForSequenceClassification were not initialized from the model checkpoint at kykim/funnel-kor-base and are newly initialized: ['classifier.linear_hidden.weight', 'classifier.linear_out.weight', 'classifier.linear_out.bias', 'classifier.linear_hidden.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Training for MAX_LEN=256:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Training for MAX_LEN=256:  50%|█████     | 1/2 [13:20<13:20, 800.21s/it]\u001b[A\n",
      "Training for MAX_LEN=256: 100%|██████████| 2/2 [26:40<00:00, 800.32s/it]\u001b[A\n",
      "Overall Progress:  39%|███▉      | 9/23 [3:05:05<5:32:38, 1425.63s/it]  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kykim/funnel-kor-base were not used when initializing FunnelForSequenceClassification: ['decoder.layers.1.attention.post_proj.bias', 'decoder.layers.0.ffn.linear_1.weight', 'decoder.layers.1.attention.seg_embed', 'decoder.layers.1.attention.k_head.bias', 'decoder.layers.0.attention.r_r_bias', 'decoder.layers.1.attention.r_r_bias', 'decoder.layers.0.attention.layer_norm.weight', 'decoder.layers.0.attention.layer_norm.bias', 'decoder.layers.1.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.weight', 'decoder.layers.0.attention.r_w_bias', 'decoder.layers.0.ffn.linear_2.bias', 'decoder.layers.0.attention.seg_embed', 'decoder.layers.1.ffn.linear_1.weight', 'decoder.layers.0.attention.q_head.weight', 'decoder.layers.1.attention.r_s_bias', 'decoder.layers.1.attention.k_head.weight', 'decoder.layers.0.attention.post_proj.bias', 'decoder.layers.0.attention.k_head.bias', 'decoder.layers.1.attention.layer_norm.weight', 'decoder.layers.0.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.bias', 'decoder.layers.1.attention.q_head.weight', 'decoder.layers.1.attention.r_w_bias', 'decoder.layers.1.attention.r_kernel', 'decoder.layers.1.ffn.layer_norm.weight', 'decoder.layers.0.ffn.linear_2.weight', 'decoder.layers.0.ffn.layer_norm.weight', 'decoder.layers.0.attention.k_head.weight', 'decoder.layers.1.attention.v_head.weight', 'decoder.layers.0.attention.post_proj.weight', 'decoder.layers.1.ffn.linear_2.weight', 'decoder.layers.1.attention.v_head.bias', 'decoder.layers.1.ffn.linear_2.bias', 'decoder.layers.0.ffn.linear_1.bias', 'decoder.layers.1.ffn.linear_1.bias', 'decoder.layers.0.attention.r_s_bias', 'decoder.layers.0.attention.r_kernel', 'decoder.layers.1.attention.post_proj.weight', 'decoder.layers.1.attention.layer_norm.bias']\n",
      "- This IS expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FunnelForSequenceClassification were not initialized from the model checkpoint at kykim/funnel-kor-base and are newly initialized: ['classifier.linear_hidden.weight', 'classifier.linear_out.weight', 'classifier.linear_out.bias', 'classifier.linear_hidden.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Training for MAX_LEN=272:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Training for MAX_LEN=272:  50%|█████     | 1/2 [14:37<14:37, 877.95s/it]\u001b[A\n",
      "Training for MAX_LEN=272: 100%|██████████| 2/2 [29:16<00:00, 878.03s/it]\u001b[A\n",
      "Overall Progress:  43%|████▎     | 10/23 [3:34:31<5:31:38, 1530.69s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.9075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kykim/funnel-kor-base were not used when initializing FunnelForSequenceClassification: ['decoder.layers.1.attention.post_proj.bias', 'decoder.layers.0.ffn.linear_1.weight', 'decoder.layers.1.attention.seg_embed', 'decoder.layers.1.attention.k_head.bias', 'decoder.layers.0.attention.r_r_bias', 'decoder.layers.1.attention.r_r_bias', 'decoder.layers.0.attention.layer_norm.weight', 'decoder.layers.0.attention.layer_norm.bias', 'decoder.layers.1.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.weight', 'decoder.layers.0.attention.r_w_bias', 'decoder.layers.0.ffn.linear_2.bias', 'decoder.layers.0.attention.seg_embed', 'decoder.layers.1.ffn.linear_1.weight', 'decoder.layers.0.attention.q_head.weight', 'decoder.layers.1.attention.r_s_bias', 'decoder.layers.1.attention.k_head.weight', 'decoder.layers.0.attention.post_proj.bias', 'decoder.layers.0.attention.k_head.bias', 'decoder.layers.1.attention.layer_norm.weight', 'decoder.layers.0.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.bias', 'decoder.layers.1.attention.q_head.weight', 'decoder.layers.1.attention.r_w_bias', 'decoder.layers.1.attention.r_kernel', 'decoder.layers.1.ffn.layer_norm.weight', 'decoder.layers.0.ffn.linear_2.weight', 'decoder.layers.0.ffn.layer_norm.weight', 'decoder.layers.0.attention.k_head.weight', 'decoder.layers.1.attention.v_head.weight', 'decoder.layers.0.attention.post_proj.weight', 'decoder.layers.1.ffn.linear_2.weight', 'decoder.layers.1.attention.v_head.bias', 'decoder.layers.1.ffn.linear_2.bias', 'decoder.layers.0.ffn.linear_1.bias', 'decoder.layers.1.ffn.linear_1.bias', 'decoder.layers.0.attention.r_s_bias', 'decoder.layers.0.attention.r_kernel', 'decoder.layers.1.attention.post_proj.weight', 'decoder.layers.1.attention.layer_norm.bias']\n",
      "- This IS expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FunnelForSequenceClassification were not initialized from the model checkpoint at kykim/funnel-kor-base and are newly initialized: ['classifier.linear_hidden.weight', 'classifier.linear_out.weight', 'classifier.linear_out.bias', 'classifier.linear_hidden.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Training for MAX_LEN=288:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Training for MAX_LEN=288:  50%|█████     | 1/2 [15:10<15:10, 910.83s/it]\u001b[A\n",
      "Training for MAX_LEN=288: 100%|██████████| 2/2 [30:22<00:00, 911.03s/it]\u001b[A\n",
      "Overall Progress:  48%|████▊     | 11/23 [4:05:03<5:24:35, 1622.95s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kykim/funnel-kor-base were not used when initializing FunnelForSequenceClassification: ['decoder.layers.1.attention.post_proj.bias', 'decoder.layers.0.ffn.linear_1.weight', 'decoder.layers.1.attention.seg_embed', 'decoder.layers.1.attention.k_head.bias', 'decoder.layers.0.attention.r_r_bias', 'decoder.layers.1.attention.r_r_bias', 'decoder.layers.0.attention.layer_norm.weight', 'decoder.layers.0.attention.layer_norm.bias', 'decoder.layers.1.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.weight', 'decoder.layers.0.attention.r_w_bias', 'decoder.layers.0.ffn.linear_2.bias', 'decoder.layers.0.attention.seg_embed', 'decoder.layers.1.ffn.linear_1.weight', 'decoder.layers.0.attention.q_head.weight', 'decoder.layers.1.attention.r_s_bias', 'decoder.layers.1.attention.k_head.weight', 'decoder.layers.0.attention.post_proj.bias', 'decoder.layers.0.attention.k_head.bias', 'decoder.layers.1.attention.layer_norm.weight', 'decoder.layers.0.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.bias', 'decoder.layers.1.attention.q_head.weight', 'decoder.layers.1.attention.r_w_bias', 'decoder.layers.1.attention.r_kernel', 'decoder.layers.1.ffn.layer_norm.weight', 'decoder.layers.0.ffn.linear_2.weight', 'decoder.layers.0.ffn.layer_norm.weight', 'decoder.layers.0.attention.k_head.weight', 'decoder.layers.1.attention.v_head.weight', 'decoder.layers.0.attention.post_proj.weight', 'decoder.layers.1.ffn.linear_2.weight', 'decoder.layers.1.attention.v_head.bias', 'decoder.layers.1.ffn.linear_2.bias', 'decoder.layers.0.ffn.linear_1.bias', 'decoder.layers.1.ffn.linear_1.bias', 'decoder.layers.0.attention.r_s_bias', 'decoder.layers.0.attention.r_kernel', 'decoder.layers.1.attention.post_proj.weight', 'decoder.layers.1.attention.layer_norm.bias']\n",
      "- This IS expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FunnelForSequenceClassification were not initialized from the model checkpoint at kykim/funnel-kor-base and are newly initialized: ['classifier.linear_hidden.weight', 'classifier.linear_out.weight', 'classifier.linear_out.bias', 'classifier.linear_hidden.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Training for MAX_LEN=304:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Training for MAX_LEN=304:  50%|█████     | 1/2 [16:15<16:15, 975.88s/it]\u001b[A\n",
      "Training for MAX_LEN=304: 100%|██████████| 2/2 [32:31<00:00, 975.84s/it]\u001b[A\n",
      "Overall Progress:  52%|█████▏    | 12/23 [4:37:46<5:16:28, 1726.21s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.8975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kykim/funnel-kor-base were not used when initializing FunnelForSequenceClassification: ['decoder.layers.1.attention.post_proj.bias', 'decoder.layers.0.ffn.linear_1.weight', 'decoder.layers.1.attention.seg_embed', 'decoder.layers.1.attention.k_head.bias', 'decoder.layers.0.attention.r_r_bias', 'decoder.layers.1.attention.r_r_bias', 'decoder.layers.0.attention.layer_norm.weight', 'decoder.layers.0.attention.layer_norm.bias', 'decoder.layers.1.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.weight', 'decoder.layers.0.attention.r_w_bias', 'decoder.layers.0.ffn.linear_2.bias', 'decoder.layers.0.attention.seg_embed', 'decoder.layers.1.ffn.linear_1.weight', 'decoder.layers.0.attention.q_head.weight', 'decoder.layers.1.attention.r_s_bias', 'decoder.layers.1.attention.k_head.weight', 'decoder.layers.0.attention.post_proj.bias', 'decoder.layers.0.attention.k_head.bias', 'decoder.layers.1.attention.layer_norm.weight', 'decoder.layers.0.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.bias', 'decoder.layers.1.attention.q_head.weight', 'decoder.layers.1.attention.r_w_bias', 'decoder.layers.1.attention.r_kernel', 'decoder.layers.1.ffn.layer_norm.weight', 'decoder.layers.0.ffn.linear_2.weight', 'decoder.layers.0.ffn.layer_norm.weight', 'decoder.layers.0.attention.k_head.weight', 'decoder.layers.1.attention.v_head.weight', 'decoder.layers.0.attention.post_proj.weight', 'decoder.layers.1.ffn.linear_2.weight', 'decoder.layers.1.attention.v_head.bias', 'decoder.layers.1.ffn.linear_2.bias', 'decoder.layers.0.ffn.linear_1.bias', 'decoder.layers.1.ffn.linear_1.bias', 'decoder.layers.0.attention.r_s_bias', 'decoder.layers.0.attention.r_kernel', 'decoder.layers.1.attention.post_proj.weight', 'decoder.layers.1.attention.layer_norm.bias']\n",
      "- This IS expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FunnelForSequenceClassification were not initialized from the model checkpoint at kykim/funnel-kor-base and are newly initialized: ['classifier.linear_hidden.weight', 'classifier.linear_out.weight', 'classifier.linear_out.bias', 'classifier.linear_hidden.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Training for MAX_LEN=320:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Training for MAX_LEN=320:  50%|█████     | 1/2 [16:51<16:51, 1011.28s/it]\u001b[A\n",
      "Training for MAX_LEN=320: 100%|██████████| 2/2 [33:42<00:00, 1011.32s/it]\u001b[A\n",
      "Overall Progress:  57%|█████▋    | 13/23 [5:11:39<5:03:12, 1819.28s/it]  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.8875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kykim/funnel-kor-base were not used when initializing FunnelForSequenceClassification: ['decoder.layers.1.attention.post_proj.bias', 'decoder.layers.0.ffn.linear_1.weight', 'decoder.layers.1.attention.seg_embed', 'decoder.layers.1.attention.k_head.bias', 'decoder.layers.0.attention.r_r_bias', 'decoder.layers.1.attention.r_r_bias', 'decoder.layers.0.attention.layer_norm.weight', 'decoder.layers.0.attention.layer_norm.bias', 'decoder.layers.1.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.weight', 'decoder.layers.0.attention.r_w_bias', 'decoder.layers.0.ffn.linear_2.bias', 'decoder.layers.0.attention.seg_embed', 'decoder.layers.1.ffn.linear_1.weight', 'decoder.layers.0.attention.q_head.weight', 'decoder.layers.1.attention.r_s_bias', 'decoder.layers.1.attention.k_head.weight', 'decoder.layers.0.attention.post_proj.bias', 'decoder.layers.0.attention.k_head.bias', 'decoder.layers.1.attention.layer_norm.weight', 'decoder.layers.0.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.bias', 'decoder.layers.1.attention.q_head.weight', 'decoder.layers.1.attention.r_w_bias', 'decoder.layers.1.attention.r_kernel', 'decoder.layers.1.ffn.layer_norm.weight', 'decoder.layers.0.ffn.linear_2.weight', 'decoder.layers.0.ffn.layer_norm.weight', 'decoder.layers.0.attention.k_head.weight', 'decoder.layers.1.attention.v_head.weight', 'decoder.layers.0.attention.post_proj.weight', 'decoder.layers.1.ffn.linear_2.weight', 'decoder.layers.1.attention.v_head.bias', 'decoder.layers.1.ffn.linear_2.bias', 'decoder.layers.0.ffn.linear_1.bias', 'decoder.layers.1.ffn.linear_1.bias', 'decoder.layers.0.attention.r_s_bias', 'decoder.layers.0.attention.r_kernel', 'decoder.layers.1.attention.post_proj.weight', 'decoder.layers.1.attention.layer_norm.bias']\n",
      "- This IS expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FunnelForSequenceClassification were not initialized from the model checkpoint at kykim/funnel-kor-base and are newly initialized: ['classifier.linear_hidden.weight', 'classifier.linear_out.weight', 'classifier.linear_out.bias', 'classifier.linear_hidden.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Training for MAX_LEN=336:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Training for MAX_LEN=336:  50%|█████     | 1/2 [18:05<18:05, 1085.91s/it]\u001b[A\n",
      "Training for MAX_LEN=336: 100%|██████████| 2/2 [36:12<00:00, 1086.23s/it]\u001b[A\n",
      "                                                                         \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  61%|██████    | 14/23 [5:48:04<4:49:28, 1929.84s/it]Some weights of the model checkpoint at kykim/funnel-kor-base were not used when initializing FunnelForSequenceClassification: ['decoder.layers.1.attention.post_proj.bias', 'decoder.layers.0.ffn.linear_1.weight', 'decoder.layers.1.attention.seg_embed', 'decoder.layers.1.attention.k_head.bias', 'decoder.layers.0.attention.r_r_bias', 'decoder.layers.1.attention.r_r_bias', 'decoder.layers.0.attention.layer_norm.weight', 'decoder.layers.0.attention.layer_norm.bias', 'decoder.layers.1.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.weight', 'decoder.layers.0.attention.r_w_bias', 'decoder.layers.0.ffn.linear_2.bias', 'decoder.layers.0.attention.seg_embed', 'decoder.layers.1.ffn.linear_1.weight', 'decoder.layers.0.attention.q_head.weight', 'decoder.layers.1.attention.r_s_bias', 'decoder.layers.1.attention.k_head.weight', 'decoder.layers.0.attention.post_proj.bias', 'decoder.layers.0.attention.k_head.bias', 'decoder.layers.1.attention.layer_norm.weight', 'decoder.layers.0.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.bias', 'decoder.layers.1.attention.q_head.weight', 'decoder.layers.1.attention.r_w_bias', 'decoder.layers.1.attention.r_kernel', 'decoder.layers.1.ffn.layer_norm.weight', 'decoder.layers.0.ffn.linear_2.weight', 'decoder.layers.0.ffn.layer_norm.weight', 'decoder.layers.0.attention.k_head.weight', 'decoder.layers.1.attention.v_head.weight', 'decoder.layers.0.attention.post_proj.weight', 'decoder.layers.1.ffn.linear_2.weight', 'decoder.layers.1.attention.v_head.bias', 'decoder.layers.1.ffn.linear_2.bias', 'decoder.layers.0.ffn.linear_1.bias', 'decoder.layers.1.ffn.linear_1.bias', 'decoder.layers.0.attention.r_s_bias', 'decoder.layers.0.attention.r_kernel', 'decoder.layers.1.attention.post_proj.weight', 'decoder.layers.1.attention.layer_norm.bias']\n",
      "- This IS expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FunnelForSequenceClassification were not initialized from the model checkpoint at kykim/funnel-kor-base and are newly initialized: ['classifier.linear_hidden.weight', 'classifier.linear_out.weight', 'classifier.linear_out.bias', 'classifier.linear_hidden.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Training for MAX_LEN=352:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Training for MAX_LEN=352:  50%|█████     | 1/2 [19:12<19:12, 1152.47s/it]\u001b[A\n",
      "Training for MAX_LEN=352: 100%|██████████| 2/2 [38:25<00:00, 1152.68s/it]\u001b[A\n",
      "Overall Progress:  65%|██████▌   | 15/23 [6:26:42<4:32:53, 2046.74s/it]  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.9075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kykim/funnel-kor-base were not used when initializing FunnelForSequenceClassification: ['decoder.layers.1.attention.post_proj.bias', 'decoder.layers.0.ffn.linear_1.weight', 'decoder.layers.1.attention.seg_embed', 'decoder.layers.1.attention.k_head.bias', 'decoder.layers.0.attention.r_r_bias', 'decoder.layers.1.attention.r_r_bias', 'decoder.layers.0.attention.layer_norm.weight', 'decoder.layers.0.attention.layer_norm.bias', 'decoder.layers.1.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.weight', 'decoder.layers.0.attention.r_w_bias', 'decoder.layers.0.ffn.linear_2.bias', 'decoder.layers.0.attention.seg_embed', 'decoder.layers.1.ffn.linear_1.weight', 'decoder.layers.0.attention.q_head.weight', 'decoder.layers.1.attention.r_s_bias', 'decoder.layers.1.attention.k_head.weight', 'decoder.layers.0.attention.post_proj.bias', 'decoder.layers.0.attention.k_head.bias', 'decoder.layers.1.attention.layer_norm.weight', 'decoder.layers.0.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.bias', 'decoder.layers.1.attention.q_head.weight', 'decoder.layers.1.attention.r_w_bias', 'decoder.layers.1.attention.r_kernel', 'decoder.layers.1.ffn.layer_norm.weight', 'decoder.layers.0.ffn.linear_2.weight', 'decoder.layers.0.ffn.layer_norm.weight', 'decoder.layers.0.attention.k_head.weight', 'decoder.layers.1.attention.v_head.weight', 'decoder.layers.0.attention.post_proj.weight', 'decoder.layers.1.ffn.linear_2.weight', 'decoder.layers.1.attention.v_head.bias', 'decoder.layers.1.ffn.linear_2.bias', 'decoder.layers.0.ffn.linear_1.bias', 'decoder.layers.1.ffn.linear_1.bias', 'decoder.layers.0.attention.r_s_bias', 'decoder.layers.0.attention.r_kernel', 'decoder.layers.1.attention.post_proj.weight', 'decoder.layers.1.attention.layer_norm.bias']\n",
      "- This IS expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FunnelForSequenceClassification were not initialized from the model checkpoint at kykim/funnel-kor-base and are newly initialized: ['classifier.linear_hidden.weight', 'classifier.linear_out.weight', 'classifier.linear_out.bias', 'classifier.linear_hidden.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Training for MAX_LEN=368:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Training for MAX_LEN=368:  50%|█████     | 1/2 [19:54<19:54, 1194.76s/it]\u001b[A\n",
      "Training for MAX_LEN=368: 100%|██████████| 2/2 [39:49<00:00, 1194.55s/it]\u001b[A\n",
      "                                                                         \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  70%|██████▉   | 16/23 [7:06:45<4:11:17, 2153.94s/it]Some weights of the model checkpoint at kykim/funnel-kor-base were not used when initializing FunnelForSequenceClassification: ['decoder.layers.1.attention.post_proj.bias', 'decoder.layers.0.ffn.linear_1.weight', 'decoder.layers.1.attention.seg_embed', 'decoder.layers.1.attention.k_head.bias', 'decoder.layers.0.attention.r_r_bias', 'decoder.layers.1.attention.r_r_bias', 'decoder.layers.0.attention.layer_norm.weight', 'decoder.layers.0.attention.layer_norm.bias', 'decoder.layers.1.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.weight', 'decoder.layers.0.attention.r_w_bias', 'decoder.layers.0.ffn.linear_2.bias', 'decoder.layers.0.attention.seg_embed', 'decoder.layers.1.ffn.linear_1.weight', 'decoder.layers.0.attention.q_head.weight', 'decoder.layers.1.attention.r_s_bias', 'decoder.layers.1.attention.k_head.weight', 'decoder.layers.0.attention.post_proj.bias', 'decoder.layers.0.attention.k_head.bias', 'decoder.layers.1.attention.layer_norm.weight', 'decoder.layers.0.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.bias', 'decoder.layers.1.attention.q_head.weight', 'decoder.layers.1.attention.r_w_bias', 'decoder.layers.1.attention.r_kernel', 'decoder.layers.1.ffn.layer_norm.weight', 'decoder.layers.0.ffn.linear_2.weight', 'decoder.layers.0.ffn.layer_norm.weight', 'decoder.layers.0.attention.k_head.weight', 'decoder.layers.1.attention.v_head.weight', 'decoder.layers.0.attention.post_proj.weight', 'decoder.layers.1.ffn.linear_2.weight', 'decoder.layers.1.attention.v_head.bias', 'decoder.layers.1.ffn.linear_2.bias', 'decoder.layers.0.ffn.linear_1.bias', 'decoder.layers.1.ffn.linear_1.bias', 'decoder.layers.0.attention.r_s_bias', 'decoder.layers.0.attention.r_kernel', 'decoder.layers.1.attention.post_proj.weight', 'decoder.layers.1.attention.layer_norm.bias']\n",
      "- This IS expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FunnelForSequenceClassification were not initialized from the model checkpoint at kykim/funnel-kor-base and are newly initialized: ['classifier.linear_hidden.weight', 'classifier.linear_out.weight', 'classifier.linear_out.bias', 'classifier.linear_hidden.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Training for MAX_LEN=384:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Training for MAX_LEN=384:  50%|█████     | 1/2 [21:06<21:06, 1266.30s/it]\u001b[A\n",
      "Training for MAX_LEN=384: 100%|██████████| 2/2 [42:11<00:00, 1265.57s/it]\u001b[A\n",
      "Overall Progress:  74%|███████▍  | 17/23 [7:49:10<3:47:08, 2271.43s/it]  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kykim/funnel-kor-base were not used when initializing FunnelForSequenceClassification: ['decoder.layers.1.attention.post_proj.bias', 'decoder.layers.0.ffn.linear_1.weight', 'decoder.layers.1.attention.seg_embed', 'decoder.layers.1.attention.k_head.bias', 'decoder.layers.0.attention.r_r_bias', 'decoder.layers.1.attention.r_r_bias', 'decoder.layers.0.attention.layer_norm.weight', 'decoder.layers.0.attention.layer_norm.bias', 'decoder.layers.1.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.weight', 'decoder.layers.0.attention.r_w_bias', 'decoder.layers.0.ffn.linear_2.bias', 'decoder.layers.0.attention.seg_embed', 'decoder.layers.1.ffn.linear_1.weight', 'decoder.layers.0.attention.q_head.weight', 'decoder.layers.1.attention.r_s_bias', 'decoder.layers.1.attention.k_head.weight', 'decoder.layers.0.attention.post_proj.bias', 'decoder.layers.0.attention.k_head.bias', 'decoder.layers.1.attention.layer_norm.weight', 'decoder.layers.0.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.bias', 'decoder.layers.1.attention.q_head.weight', 'decoder.layers.1.attention.r_w_bias', 'decoder.layers.1.attention.r_kernel', 'decoder.layers.1.ffn.layer_norm.weight', 'decoder.layers.0.ffn.linear_2.weight', 'decoder.layers.0.ffn.layer_norm.weight', 'decoder.layers.0.attention.k_head.weight', 'decoder.layers.1.attention.v_head.weight', 'decoder.layers.0.attention.post_proj.weight', 'decoder.layers.1.ffn.linear_2.weight', 'decoder.layers.1.attention.v_head.bias', 'decoder.layers.1.ffn.linear_2.bias', 'decoder.layers.0.ffn.linear_1.bias', 'decoder.layers.1.ffn.linear_1.bias', 'decoder.layers.0.attention.r_s_bias', 'decoder.layers.0.attention.r_kernel', 'decoder.layers.1.attention.post_proj.weight', 'decoder.layers.1.attention.layer_norm.bias']\n",
      "- This IS expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FunnelForSequenceClassification were not initialized from the model checkpoint at kykim/funnel-kor-base and are newly initialized: ['classifier.linear_hidden.weight', 'classifier.linear_out.weight', 'classifier.linear_out.bias', 'classifier.linear_hidden.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Training for MAX_LEN=400:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Training for MAX_LEN=400:  50%|█████     | 1/2 [22:00<22:00, 1320.82s/it]\u001b[A\n",
      "Training for MAX_LEN=400: 100%|██████████| 2/2 [44:01<00:00, 1320.65s/it]\u001b[A\n",
      "                                                                         \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.9125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:  78%|███████▊  | 18/23 [8:33:26<3:18:55, 2387.05s/it]Some weights of the model checkpoint at kykim/funnel-kor-base were not used when initializing FunnelForSequenceClassification: ['decoder.layers.1.attention.post_proj.bias', 'decoder.layers.0.ffn.linear_1.weight', 'decoder.layers.1.attention.seg_embed', 'decoder.layers.1.attention.k_head.bias', 'decoder.layers.0.attention.r_r_bias', 'decoder.layers.1.attention.r_r_bias', 'decoder.layers.0.attention.layer_norm.weight', 'decoder.layers.0.attention.layer_norm.bias', 'decoder.layers.1.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.weight', 'decoder.layers.0.attention.r_w_bias', 'decoder.layers.0.ffn.linear_2.bias', 'decoder.layers.0.attention.seg_embed', 'decoder.layers.1.ffn.linear_1.weight', 'decoder.layers.0.attention.q_head.weight', 'decoder.layers.1.attention.r_s_bias', 'decoder.layers.1.attention.k_head.weight', 'decoder.layers.0.attention.post_proj.bias', 'decoder.layers.0.attention.k_head.bias', 'decoder.layers.1.attention.layer_norm.weight', 'decoder.layers.0.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.bias', 'decoder.layers.1.attention.q_head.weight', 'decoder.layers.1.attention.r_w_bias', 'decoder.layers.1.attention.r_kernel', 'decoder.layers.1.ffn.layer_norm.weight', 'decoder.layers.0.ffn.linear_2.weight', 'decoder.layers.0.ffn.layer_norm.weight', 'decoder.layers.0.attention.k_head.weight', 'decoder.layers.1.attention.v_head.weight', 'decoder.layers.0.attention.post_proj.weight', 'decoder.layers.1.ffn.linear_2.weight', 'decoder.layers.1.attention.v_head.bias', 'decoder.layers.1.ffn.linear_2.bias', 'decoder.layers.0.ffn.linear_1.bias', 'decoder.layers.1.ffn.linear_1.bias', 'decoder.layers.0.attention.r_s_bias', 'decoder.layers.0.attention.r_kernel', 'decoder.layers.1.attention.post_proj.weight', 'decoder.layers.1.attention.layer_norm.bias']\n",
      "- This IS expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FunnelForSequenceClassification were not initialized from the model checkpoint at kykim/funnel-kor-base and are newly initialized: ['classifier.linear_hidden.weight', 'classifier.linear_out.weight', 'classifier.linear_out.bias', 'classifier.linear_hidden.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Training for MAX_LEN=416:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Training for MAX_LEN=416:  50%|█████     | 1/2 [22:58<22:58, 1378.11s/it]\u001b[A\n",
      "Training for MAX_LEN=416: 100%|██████████| 2/2 [45:56<00:00, 1378.09s/it]\u001b[A\n",
      "Overall Progress:  83%|████████▎ | 19/23 [9:19:58<2:47:14, 2508.64s/it]  \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kykim/funnel-kor-base were not used when initializing FunnelForSequenceClassification: ['decoder.layers.1.attention.post_proj.bias', 'decoder.layers.0.ffn.linear_1.weight', 'decoder.layers.1.attention.seg_embed', 'decoder.layers.1.attention.k_head.bias', 'decoder.layers.0.attention.r_r_bias', 'decoder.layers.1.attention.r_r_bias', 'decoder.layers.0.attention.layer_norm.weight', 'decoder.layers.0.attention.layer_norm.bias', 'decoder.layers.1.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.weight', 'decoder.layers.0.attention.r_w_bias', 'decoder.layers.0.ffn.linear_2.bias', 'decoder.layers.0.attention.seg_embed', 'decoder.layers.1.ffn.linear_1.weight', 'decoder.layers.0.attention.q_head.weight', 'decoder.layers.1.attention.r_s_bias', 'decoder.layers.1.attention.k_head.weight', 'decoder.layers.0.attention.post_proj.bias', 'decoder.layers.0.attention.k_head.bias', 'decoder.layers.1.attention.layer_norm.weight', 'decoder.layers.0.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.bias', 'decoder.layers.1.attention.q_head.weight', 'decoder.layers.1.attention.r_w_bias', 'decoder.layers.1.attention.r_kernel', 'decoder.layers.1.ffn.layer_norm.weight', 'decoder.layers.0.ffn.linear_2.weight', 'decoder.layers.0.ffn.layer_norm.weight', 'decoder.layers.0.attention.k_head.weight', 'decoder.layers.1.attention.v_head.weight', 'decoder.layers.0.attention.post_proj.weight', 'decoder.layers.1.ffn.linear_2.weight', 'decoder.layers.1.attention.v_head.bias', 'decoder.layers.1.ffn.linear_2.bias', 'decoder.layers.0.ffn.linear_1.bias', 'decoder.layers.1.ffn.linear_1.bias', 'decoder.layers.0.attention.r_s_bias', 'decoder.layers.0.attention.r_kernel', 'decoder.layers.1.attention.post_proj.weight', 'decoder.layers.1.attention.layer_norm.bias']\n",
      "- This IS expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FunnelForSequenceClassification were not initialized from the model checkpoint at kykim/funnel-kor-base and are newly initialized: ['classifier.linear_hidden.weight', 'classifier.linear_out.weight', 'classifier.linear_out.bias', 'classifier.linear_hidden.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Training for MAX_LEN=432:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Training for MAX_LEN=432:  50%|█████     | 1/2 [24:26<24:26, 1466.30s/it]\u001b[A\n",
      "Training for MAX_LEN=432: 100%|██████████| 2/2 [48:53<00:00, 1466.78s/it]\u001b[A\n",
      "Overall Progress:  87%|████████▋ | 20/23 [10:09:06<2:12:02, 2640.75s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.9025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kykim/funnel-kor-base were not used when initializing FunnelForSequenceClassification: ['decoder.layers.1.attention.post_proj.bias', 'decoder.layers.0.ffn.linear_1.weight', 'decoder.layers.1.attention.seg_embed', 'decoder.layers.1.attention.k_head.bias', 'decoder.layers.0.attention.r_r_bias', 'decoder.layers.1.attention.r_r_bias', 'decoder.layers.0.attention.layer_norm.weight', 'decoder.layers.0.attention.layer_norm.bias', 'decoder.layers.1.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.weight', 'decoder.layers.0.attention.r_w_bias', 'decoder.layers.0.ffn.linear_2.bias', 'decoder.layers.0.attention.seg_embed', 'decoder.layers.1.ffn.linear_1.weight', 'decoder.layers.0.attention.q_head.weight', 'decoder.layers.1.attention.r_s_bias', 'decoder.layers.1.attention.k_head.weight', 'decoder.layers.0.attention.post_proj.bias', 'decoder.layers.0.attention.k_head.bias', 'decoder.layers.1.attention.layer_norm.weight', 'decoder.layers.0.ffn.layer_norm.bias', 'decoder.layers.0.attention.v_head.bias', 'decoder.layers.1.attention.q_head.weight', 'decoder.layers.1.attention.r_w_bias', 'decoder.layers.1.attention.r_kernel', 'decoder.layers.1.ffn.layer_norm.weight', 'decoder.layers.0.ffn.linear_2.weight', 'decoder.layers.0.ffn.layer_norm.weight', 'decoder.layers.0.attention.k_head.weight', 'decoder.layers.1.attention.v_head.weight', 'decoder.layers.0.attention.post_proj.weight', 'decoder.layers.1.ffn.linear_2.weight', 'decoder.layers.1.attention.v_head.bias', 'decoder.layers.1.ffn.linear_2.bias', 'decoder.layers.0.ffn.linear_1.bias', 'decoder.layers.1.ffn.linear_1.bias', 'decoder.layers.0.attention.r_s_bias', 'decoder.layers.0.attention.r_kernel', 'decoder.layers.1.attention.post_proj.weight', 'decoder.layers.1.attention.layer_norm.bias']\n",
      "- This IS expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FunnelForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of FunnelForSequenceClassification were not initialized from the model checkpoint at kykim/funnel-kor-base and are newly initialized: ['classifier.linear_hidden.weight', 'classifier.linear_out.weight', 'classifier.linear_out.bias', 'classifier.linear_hidden.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Training for MAX_LEN=448:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Overall Progress:  87%|████████▋ | 20/23 [10:09:11<1:31:22, 1827.57s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.76 GiB total capacity; 13.09 GiB already allocated; 3.75 MiB free; 13.34 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_33/1129259181.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'targets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunnel_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_33/2612697410.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, ids, mask)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/models/funnel/modeling_funnel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m         outputs = self.funnel(\n\u001b[0m\u001b[1;32m   1276\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/models/funnel/modeling_funnel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    957\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/models/funnel/modeling_funnel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs_embeds, attention_mask, token_type_ids, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    649\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m                         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m                     \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m                     \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mdo_pooling\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/models/funnel/modeling_funnel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, attention_inputs, output_attentions)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m         \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_attentions\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/models/funnel/modeling_funnel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, attention_inputs, output_attentions)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# merge attention scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mattn_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontent_score\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpositional_attn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtoken_type_attn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0;31m# precision safe in case of mixed precision training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.76 GiB total capacity; 13.09 GiB already allocated; 3.75 MiB free; 13.34 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "\n",
    "# 토큰 최대 길이를 128에서 432까지 16씩 증가시키며 조정\n",
    "for MAX_LEN in tqdm(range(128, 433, 16), desc=\"Overall Progress\"):\n",
    "    funnel_train_dataset = FunnelDataset(merged_data, funnel_tokenizer, MAX_LEN)\n",
    "    funnel_train_data_loader = DataLoader(funnel_train_dataset, batch_size=16)\n",
    "    \n",
    "    funnel_model = FunnelKorBaseClass()\n",
    "    funnel_model.to(torch.device(\"cuda\"))\n",
    "    \n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    funnel_optimizer = torch.optim.Adam(params=funnel_model.parameters(), lr=1e-5)\n",
    "    \n",
    "    for epoch in tqdm(range(2), desc=f\"Training for MAX_LEN={MAX_LEN}\", leave=False):\n",
    "        funnel_model.train()\n",
    "        for _, data in enumerate(funnel_train_data_loader, start=1):\n",
    "            ids = data['ids'].to(torch.device(\"cuda\"), dtype=torch.long)\n",
    "            mask = data['mask'].to(torch.device(\"cuda\"), dtype=torch.long)\n",
    "            targets = data['targets'].to(torch.device(\"cuda\"), dtype=torch.long)\n",
    "            \n",
    "            outputs = funnel_model(ids, mask)\n",
    "            loss = loss_function(outputs, targets)\n",
    "            \n",
    "            funnel_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            funnel_optimizer.step()\n",
    "    \n",
    "    # test.json을 사용하여 모델 평가하고 submission.csv에 결과 저장\n",
    "    test_dataset = FunnelDataset(test, funnel_tokenizer, MAX_LEN)\n",
    "    test_data_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "    test_predictions = predict(funnel_model, test_data_loader)\n",
    "\n",
    "    submission_df = pd.DataFrame(test_predictions, columns=['class'])\n",
    "    submission_df.to_csv(f\"./data/funnel/submission_{MAX_LEN}.csv\", index=False)\n",
    "    \n",
    "    # submission.csv와 answer.csv를 비교하여 정확도 계산\n",
    "    answer_df = pd.read_csv(\"./data/answer.csv\")\n",
    "    correct_predictions = (submission_df['class'] == answer_df['class']).sum()\n",
    "    accuracy = correct_predictions / len(answer_df)\n",
    "    print(f'accuracy = {accuracy}')\n",
    "    \n",
    "    results.append(accuracy)\n",
    "    \n",
    "    if accuracy >= 0.91:\n",
    "        model_name = f\"funnel_{accuracy:.4f}_{MAX_LEN}\"\n",
    "        torch.save(funnel_model.state_dict(), f\"./data/funnel/{model_name}.pth\")\n",
    "\n",
    "results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6126a538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (23,) and (20,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_33/445827877.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Plot the accuracy values for each MAX_LEN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMAX_LEN_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MAX_LEN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3017\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3018\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3019\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   3020\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \"\"\"\n\u001b[1;32m   1604\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1605\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    502\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (23,) and (20,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the range of MAX_LEN values\n",
    "MAX_LEN_values = list(range(128, 481, 16))\n",
    "\n",
    "# Plot the accuracy values for each MAX_LEN\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(MAX_LEN_values, results, marker='o')\n",
    "plt.xlabel('MAX_LEN')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs. MAX_LEN')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bb75f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
